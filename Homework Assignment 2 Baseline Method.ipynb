{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import sklearn\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download files, set up folder, put files into folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data: ./train.tsv\n",
    "# test data:     ./test.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       label                                             review\n",
      "0          0  Leaks: Liss seems to be totally incompetent: m...\n",
      "1          1  Replacement Peeler: Loved my old one. Loaned i...\n",
      "2          0  Not what I was expecting: I chose to rate this...\n",
      "3          1  Watch face is hard to read: Although I don't o...\n",
      "4          0  Disappointing: I was eager to read this book s...\n",
      "...      ...                                                ...\n",
      "29991      1  Love EW: I must admit that I am a total TV afi...\n",
      "29992      1  Easy to follow and delicious recipes!: I compl...\n",
      "29993      1  The Beauty and Mystery of Veronique: Perhaps t...\n",
      "29994      1  I love it.: Brilliant, hilarious, quick and ea...\n",
      "29995      0  broken...: bad choice...2d film would not play...\n",
      "\n",
      "[29996 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "dataframe = pd.read_csv('./train.tsv', sep = '\\t')\n",
    "print(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set size: 23997\n",
      "validation set size: 5999\n"
     ]
    }
   ],
   "source": [
    "train_ratio = 0.8 # 80% for training, 20% for validation\n",
    "random_seed = 100\n",
    "\n",
    "train_dataframe = dataframe.sample(frac=train_ratio, random_state=random_seed)\n",
    "valid_dataframe = dataframe.drop(train_dataframe.index)\n",
    "print('training set size:', len(train_dataframe))\n",
    "print('validation set size:', len(valid_dataframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id                                             review\n",
      "0        1  Human Hurricane!: Would you like to sleep in t...\n",
      "1        2  A Mom: I bought this with all kinds of expecta...\n",
      "2        3  Good Read: I judge all books that I read by a ...\n",
      "3        4  It's awesome: DVD set is exactly what you'd bu...\n",
      "4        5  Great Movie!!!: This definatly the best Godzil...\n",
      "...    ...                                                ...\n",
      "5995  5996  Beautiful and Spiritual: This is a very beauti...\n",
      "5996  5997  Another Cash In: This cd is pure dreck and it'...\n",
      "5997  5998  Concept drawings-very good: The concept drawin...\n",
      "5998  5999  I hear i all the time is awsome: this is great...\n",
      "5999  6000  Not so great Performance: This mouse is very s...\n",
      "\n",
      "[6000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "test_dataframe = pd.read_csv('./test.tsv', sep = '\\t')\n",
    "print (test_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try the trivial baseline: predict the majority label of the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 11965, 1: 12032})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(train_dataframe['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority guess accuracy: 0.5099183197199533\n"
     ]
    }
   ],
   "source": [
    "# Looks like label 1 has slightly more counts than label 0 in training data\n",
    "# So the 'majority guess' prediction is an array filled with 1s\n",
    "majority_guess_pred = [1 for i in range(len(valid_dataframe))]\n",
    "accuracy = accuracy_score(valid_dataframe['label'], majority_guess_pred)\n",
    "print ('Majority guess accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function: write out prediction values into a csv format file\n",
    "# params:\n",
    "#     df: dataframe, where each row is a test example, with column 'id' as data id\n",
    "#     pred: a list or 1-d array of prediction values\n",
    "#     filepath: the output file path\n",
    "# return:\n",
    "#     None\n",
    "\n",
    "def write_test_prediction(df, pred, filepath):\n",
    "    with open(filepath, 'w') as outfile:\n",
    "        outfile.write('{},{}\\n'.format('id', 'label'))\n",
    "        for index, row in df.iterrows():\n",
    "            outfile.write('{},{}\\n'.format(row['id'], pred[index]))\n",
    "    print (len(df), 'predictions are written to', filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000 predictions are written to ./majority_guess.csv\n"
     ]
    }
   ],
   "source": [
    "majority_guess_pred_test = [1 for i in range(len(test_dataframe))]\n",
    "write_test_prediction(test_dataframe, majority_guess_pred_test, './majority_guess.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build feature extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## use all unigrams from training data as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(binary=True, max_df=0.95, min_df=2, ngram_range=(1, 2))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1, 2), max_df=0.95, min_df=2, binary=True)\n",
    "vectorizer.fit(train_dataframe['review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract feature vectors for training, validation, and test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23997, 166464)\n",
      "(5999, 166464)\n",
      "(6000, 166464)\n"
     ]
    }
   ],
   "source": [
    "train_X = vectorizer.transform(train_dataframe['review'])\n",
    "valid_X = vectorizer.transform(valid_dataframe['review'])\n",
    "test_X = vectorizer.transform(test_dataframe['review'])\n",
    "print (train_X.shape)\n",
    "print (valid_X.shape)\n",
    "print (test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23997,)\n",
      "(5999,)\n"
     ]
    }
   ],
   "source": [
    "train_y = train_dataframe['label'].to_numpy()\n",
    "valid_y = valid_dataframe['label'].to_numpy()\n",
    "print (train_y.shape)\n",
    "print (valid_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use chi-square statistic to select a subset of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features_to_select = 5000\n",
    "feature_selector = SelectKBest(score_func = chi2, k = num_features_to_select)\n",
    "feature_selector.fit(train_X, train_y)\n",
    "\n",
    "# feature names\n",
    "all_features = [feature for feature, index in sorted(vectorizer.vocabulary_.items(), key = lambda x: x[1])]\n",
    "selected_features = feature_selector.get_feature_names_out(input_features = all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23997, 5000)\n",
      "(5999, 5000)\n",
      "(6000, 5000)\n"
     ]
    }
   ],
   "source": [
    "train_X_selected = feature_selector.transform(train_X)\n",
    "valid_X_selected = feature_selector.transform(valid_X)\n",
    "test_X_selected = feature_selector.transform(test_X)\n",
    "print (train_X_selected.shape)\n",
    "print (valid_X_selected.shape)\n",
    "print (test_X_selected.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model on training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, solver='liblinear')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(C = 1, solver='liblinear')\n",
    "model.fit(train_X_selected, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model on training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression, accuracy on training set: 0.9598699837479685\n"
     ]
    }
   ],
   "source": [
    "train_y_hat = model.predict(train_X_selected)\n",
    "accuracy = accuracy_score(train_y, train_y_hat)\n",
    "print ('Logistic regression, accuracy on training set:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression, accuracy on validation set: 0.9034839139856643\n"
     ]
    }
   ],
   "source": [
    "valid_y_hat = model.predict(valid_X_selected)\n",
    "accuracy = accuracy_score(valid_y, valid_y_hat)\n",
    "print ('Logistic regression, accuracy on validation set:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After experimentation on the validation set: retrain the final model on all training data, and predict labels for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000 predictions are written to ./logistic_regression.csv\n"
     ]
    }
   ],
   "source": [
    "all_train_X = vectorizer.transform(dataframe['review'])\n",
    "all_train_X_selected = feature_selector.transform(all_train_X)\n",
    "all_train_y = dataframe['label'].to_numpy()\n",
    "\n",
    "model.fit(all_train_X_selected, all_train_y)\n",
    "test_y_hat = model.predict(test_X_selected)\n",
    "write_test_prediction(test_dataframe, test_y_hat, './logistic_regression.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigate what the model has learned and where it failed (A.K.A. error analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at learned parameters (for linear model: weight of each dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a mapping: word -> learned weight of this word\n",
    "feature_weight = {}\n",
    "for idx, feature in enumerate(selected_features):\n",
    "    feature_weight[feature] = model.coef_[0][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"yet but\" 2.0904161153475957\n",
      "\"refreshing\" 2.0898236877410414\n",
      "\"just what\" 2.0805227872007483\n",
      "\"only problem\" 2.010323911102065\n",
      "\"hooked\" 1.9207650463988215\n",
      "\"awesome\" 1.9020337563727694\n",
      "\"own the\" 1.8899042575333134\n",
      "\"worried\" 1.8751775423358847\n",
      "\"not disappointed\" 1.872033346153347\n",
      "\"is must\" 1.8162417119208776\n",
      "\"loves this\" 1.7965671777130785\n",
      "\"love it\" 1.7899655940934682\n",
      "\"pleasantly\" 1.695711751891211\n",
      "\"very interesting\" 1.682476658140149\n",
      "\"neat\" 1.6595996077160493\n",
      "\"love this\" 1.6070021740522649\n",
      "\"convenient\" 1.5992719620885774\n",
      "\"rocks\" 1.5832041915767194\n",
      "\"from getting\" 1.578325248557814\n",
      "\"whether you\" 1.5613536637847947\n"
     ]
    }
   ],
   "source": [
    "# words correlated with positive sentiment (top ones)\n",
    "for k, v in sorted(feature_weight.items(), key = lambda x: x[1], reverse = True)[:20]:\n",
    "     print ('\"{}\"'.format(k), v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"not good\" -2.8426231314306194\n",
      "\"not recommend\" -2.5723959827292164\n",
      "\"two stars\" -2.517774482607159\n",
      "\"not worth\" -2.4606801128022737\n",
      "\"disappointing\" -2.4519542898454447\n",
      "\"worthless\" -2.264593724686156\n",
      "\"alas\" -2.2634422710315194\n",
      "\"overrated\" -2.2535685985667175\n",
      "\"terrible\" -2.1549726479119236\n",
      "\"poor\" -1.987966027010185\n",
      "\"worst\" -1.946719402071601\n",
      "\"unsatisfying\" -1.9420694488164494\n",
      "\"item because\" -1.9223714504479739\n",
      "\"dissapointing\" -1.9010157512915464\n",
      "\"trash\" -1.8674090261358898\n",
      "\"uninteresting\" -1.8670661845335175\n",
      "\"only good\" -1.7894340178773542\n",
      "\"pass on\" -1.7884616191289573\n",
      "\"awful\" -1.7808215225283373\n",
      "\"save your\" -1.7758026827127222\n"
     ]
    }
   ],
   "source": [
    "# words correlated with negative sentiments (top ones)\n",
    "for k, v in sorted(feature_weight.items(), key = lambda x: x[1], reverse = False)[:20]:\n",
    "     print ('\"{}\"'.format(k), v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at how the model makes predictions on individual examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We pick a set of examples from the validation set (we predicted scores for those).\n",
    "# We usually we don't pick from training data (since the good performance may be unrealistic).\n",
    "# We cannot do error analysis on test data （because no true target value is provided）."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_linear_prediction(df, model, idx2feature, X, y, y_hat, idx_list):\n",
    "    print('indices:', idx_list)\n",
    "    for idx in idx_list:\n",
    "        print ('==============', idx, '================')\n",
    "        print ('document:', df.iloc[idx]['review'])\n",
    "        print ('TRUE label:', df.iloc[idx]['label'])\n",
    "        print ('PRED label:', y_hat[idx])\n",
    "        \n",
    "        print ('\\nPRED breakdown:')\n",
    "        print ('\\tINTERCEPT', model.intercept_)\n",
    "        if X[idx, :].nnz == 0:\n",
    "            print ('\\tFEATURE', '[EMPTY]')\n",
    "        else:\n",
    "            sp_row = X[idx, :]\n",
    "            for i in range(sp_row.getnnz()): # looping over a row in sparse matrix \n",
    "                feature_value = sp_row.data[i]\n",
    "                feature_dim = sp_row.indices[i]\n",
    "                print ('\\tFEATURE', idx2feature[feature_dim], ':', feature_value, '*', model.coef_[0][feature_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indices: [4306]\n",
      "============== 4306 ================\n",
      "document: A Far Cry from Better Work: Read \"Working\" by Studs Terkel.Barbara Ehrenreich's journalistic method is biased and flawed, her intentions are good but this subject is wasted on her talents and approach. Barbara's too-late realization of what it's like to work a minimum wage job is what fostered her spewing of a laundry list of complaints with little or no advice on how to alter the system she abhors. For someone who doesn't know how to do her homework, one would have expected Barbara to have worked more of these jobs before she was 18, and I know plenty of 18-year-olds who know how to yammer better.\n",
      "TRUE label: 0\n",
      "PRED label: 1\n",
      "\n",
      "PRED breakdown:\n",
      "\tINTERCEPT [0.01566016]\n",
      "\tFEATURE and : 1 * 0.3282098284948644\n",
      "\tFEATURE approach : 1 * -0.07577257963029782\n",
      "\tFEATURE before : 1 * 0.1302199030770126\n",
      "\tFEATURE before she : 1 * -0.7404295562120137\n",
      "\tFEATURE better : 1 * -0.2808338279875675\n",
      "\tFEATURE but : 1 * 0.04668107626324463\n",
      "\tFEATURE but this : 1 * -0.4031311161522661\n",
      "\tFEATURE complaints : 1 * 0.5971215813192032\n",
      "\tFEATURE do : 1 * -0.08923916960707343\n",
      "\tFEATURE doesn : 1 * -0.5683355066244233\n",
      "\tFEATURE expected : 1 * -0.1974018745706375\n",
      "\tFEATURE good : 1 * 0.6153537978199318\n",
      "\tFEATURE have : 1 * 0.003914044745587841\n",
      "\tFEATURE her : 1 * -0.07824756129739188\n",
      "\tFEATURE is : 1 * -0.05238488342677054\n",
      "\tFEATURE it : 1 * 0.0754068285973114\n",
      "\tFEATURE job : 1 * 0.6120007063615726\n",
      "\tFEATURE like : 1 * 0.26356354112261204\n",
      "\tFEATURE little : 1 * 0.24784774458077524\n",
      "\tFEATURE no : 1 * -0.3511215967253175\n",
      "\tFEATURE on : 1 * -0.0056773159244987445\n",
      "\tFEATURE or : 1 * 0.0035039028367034597\n",
      "\tFEATURE read : 1 * 0.06777201767891226\n",
      "\tFEATURE she : 1 * 0.03828750372904189\n",
      "\tFEATURE someone : 1 * -0.3095565377254247\n",
      "\tFEATURE the : 1 * -0.15156012289343815\n",
      "\tFEATURE to work : 1 * -0.23953388504426917\n",
      "\tFEATURE too : 1 * -0.2800432566407905\n",
      "\tFEATURE was : 1 * -0.05538703527460831\n",
      "\tFEATURE wasted : 1 * -1.0416682248133065\n",
      "\tFEATURE what : 1 * -0.1500732797112376\n",
      "\tFEATURE what it : 1 * 0.795246522600392\n",
      "\tFEATURE who : 1 * -0.0290487088114385\n",
      "\tFEATURE work : 1 * 0.03427364092973917\n",
      "\tFEATURE worked : 1 * 0.269511851586797\n",
      "\tFEATURE working : 1 * -0.09220105824572074\n",
      "\tFEATURE would : 1 * -0.10650217419357164\n",
      "\tFEATURE would have : 1 * 0.11861811326461198\n"
     ]
    }
   ],
   "source": [
    "# construct a dictionary mapping: feature index -> word\n",
    "idx2feature = dict([(idx, feature) for idx, feature in enumerate(selected_features)])\n",
    "\n",
    "# look at data with prediction error\n",
    "error_indices  = [i for i in range(len(valid_y_hat)) if valid_y_hat[i] != valid_y[i]]\n",
    "explain_linear_prediction(valid_dataframe, model, idx2feature, valid_X_selected, valid_y, valid_y_hat, np.random.choice(error_indices, size = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
